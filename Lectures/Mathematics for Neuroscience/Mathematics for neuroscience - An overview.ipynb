{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mathematics for neuroscience - An overview\n",
    "**Lecture by Áine Byrne**\n",
    "\n",
    "**October 25 2021**\n",
    "\n",
    "**For the *Computational Neuroscience, Neurotechnology and Neuro-inspired Artificial Intelligence* Autumn School**\n",
    "\n",
    "\n",
    "\n",
    "# Introduction\n",
    "\n",
    "Mathematics has been of crucial importance in solving many historical challenges, particularly in the fields of physics and engineering, where mathematical concepts are regularly employed to address problems far beyond the context in which they were originally developed. More recently, mathematical models have been employed to solve problems in the realm of neuroscience, and biology more generally. Mathematical models of the brain make it possible to gain a deep and long-lasting insight into how the brain works.\n",
    "\n",
    "In this lecture, we will review the key mathematical tools needed to develop such models of the brain. These tools include differential equations, linear algebra, and numerical analysis."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hodgkin and Huxley\n",
    "\n",
    "In 1963, Alan Hodgkin and Andrew Huxley were awarded the Nobel Prize in Physiology or Medicine for their discoveries concerning the ionic mechanisms involved in excitation and inhibition in the peripheral and central portions of the nerve cell membrane\". Using a combination of electrophysiological recordings and mathematical intuition, they developed a mathematical description of how action potentials are initiate and propagate along a squid giant axon. Their work revolutionised neuroscience research and initiated a new field: mathematical neuroscience.\n",
    "\n",
    "Hodgkin and Huxley's mathematical description consisted of four ordinary differential equations, prescribing the rate of change of the membrane potential ($V$) and the 3 additional quantities related to the potassium channel activation ($n$), sodium channel activation($m$), and sodium channel inactivation ($h$).\n",
    "\\begin{align}&C_m\\frac{{\\rm d}V}{{\\rm d}t} = I - g_{Na} m^3h (V-V_{Na}) - g_K n^4 (V-V_K) - g_l (V-V_l) \\\\\n",
    "&\\tau_n(V)\\frac{{\\rm d}n}{{\\rm d}t} = n_{\\infty}(V)-n \\\\\n",
    "&\\tau_m(V)\\frac{{\\rm d}m}{{\\rm d}t} = m_{\\infty}(V)-m \\\\\n",
    "&\\tau_h(V)\\frac{{\\rm d}h}{{\\rm d}t} = h_{\\infty}(V)-h \\end{align}\n",
    "\n",
    "The Wikipedia article on the [Hodgkin and Huxley model](https://en.wikipedia.org/wiki/Hodgkin%E2%80%93Huxley_model) provides a good overview of the model and its development.\n",
    "\n",
    "Before we can study the equations of Hodgkin and Huxley, we must first learn about differential equations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differential equations\n",
    "\n",
    "A differential equation prescribes the rate of change of a particular quantity. Consider the expression for unrestricted population growth $$\\frac{{\\rm d}N}{{\\rm d}t}=a N,$$\n",
    "where $N$ is the size of a given population, $t$ is time and $a$ is a parameter describing the growth rate.\n",
    "The derivative $\\frac{{\\rm d}N}{{\\rm d}t}$ refers to the rate of change of $N$ as $t$ is varied, i.e. how is the population size going to change over time. On the right-hand side of the equation, we have an expression that prescribes that change. If $a$ is a positive, our rate of change $\\frac{{\\rm d}N}{{\\rm d}t}$ will be positive, i.e. the population size is going to increase. Whereas if $a$ is a negative, the rate of change $\\frac{{\\rm d}N}{{\\rm d}t}$ will be negative and the population size will decrease. Notice that there is also a $N$ on the right-hand side of the equation. So, if $N$ is small, the amount the population increases/decreases by is also going to be small, but the larger $N$ gets the larger increase/decrease will become.\n",
    "\n",
    "It may help to think about this in a *discretised* manner, e.g. how much do we expect the population to increase each year. Imagine the population of a particular town is 10,000 and it increases by $0.1\\times N$ each year. The change in population size this year will be $0.1\\times 10000 = 1000$, so next year the population size will be 11,000. Now applying the same logic, the population in two years time will be $11000 + 0.1\\times 11000 = 12100$. Below is a piece of code to apply this same logic to compute the population size for the next 25 years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10000\n",
    "a = 0.1\n",
    "\n",
    "print('Year   ','dN/dt    ','N')\n",
    "for i in range(1,26):\n",
    "    print(i, '     ',round(a*N), '    ', round(N+a*N))\n",
    "    N = N + a*N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1:** What happens if we change $a$ to be negative?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To study differential equations numerically, we need to manipulate and store arrays of numbers, which brings us to *linear algebra*. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear algebra\n",
    "\n",
    "Linear algebra allows us to perform mathematical operations on arrays of numbers. Computational neuroscience, and computation more generally, relies heavily on linear algebra. The basic building block of linear algebra are vectors and matrices.\n",
    "\n",
    "In this lecture will only cover the basics of linear algebra, if you would like to learn more, I recommend the [Khan Academy course on linear algebra](https://www.khanacademy.org/math/linear-algebra)\n",
    "\n",
    "## Vectors\n",
    "Vectors are essentially lists of numbers. Mathematically speaking, an $n$-dimensional vector ($n$ numbers in the list) refers to a coordinate in $n$-dimensional space. For example, if we define a vector\n",
    "$$v=\\begin{bmatrix}x \\\\ y\\end{bmatrix},$$\n",
    "$x$ is the amount we move in one direction and $y$ is the amount we move in a perpendicular direction. \n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/87/Vector_components.svg/1200px-Vector_components.svg.png\" width=400 />\n",
    "\n",
    "Python relies on a package called NumPy for linear algebra. Below is code for importing the NumPy package and creating a simple vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = np.array([1,2,3,4,5])\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Python indexing starts a zero, so the first number in our vector is entry $0$, the second is entry $1$ and so on. To access specific entries, we use square brackets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The colon operator can be used to access multiple entries. This is known as *slicing*. Run the code below to see how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic operations\n",
    "#### Scalar multiplication\n",
    "A scalar is a single number, and scalar multiplication refers to multiplying a vector by a single number. With scalar multiplication, every entry is multiplied by this number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v*2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Addition\n",
    "To add two vectors they must be the same length. The addition is performed element-by-element, i.e. the first element of vector one is added to the first element of vector two, the second element of vector one is added to the second element of vector two, and so on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = np.array([3,7,1,6,4])\n",
    "v+u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2:** Add the vectors $$a = \\begin{bmatrix}5 \\\\ 1 \\\\ 9 \\\\ 3 \\\\ 7\\end{bmatrix}\\text{, }b = \\begin{bmatrix}7 \\\\ 3 \\\\ 1 \\\\ 4 \\\\ 6\\end{bmatrix}$$by hand and then use Python to check your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dot product \n",
    "The dot product of two vectors is computed by performing element-by element multiplication and adding up all of the products. $$u\\cdot v = u_1v_1 + u_2v_2 + \\dots +u_nv_n .$$ As with adding two vectors, the two vectors must be the same length. To compute the dot product in Python we use the `dot()` function/method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v.dot(u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Using `v*u` will perform element-by-element multiplication, but not sum up the products."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3:** Compute the dot product of the vectors $a$ and $b$ (given in Exercise 2) by hand. Then compute the dot product in Python. Do the two answers match?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrices\n",
    "\n",
    "A matrix can be thought of as a collection of vectors of the same length. An $n\\times m$ matrix is a rectangular array of numbers with $n$ rows and $m$ columns. For example,\n",
    "$$A=\\begin{bmatrix}2 & 8 & 4 \\\\ 1 & 0 & 3 \\\\ 5 & 1 & 6 \\\\ 8 & 3& 5\\end{bmatrix} \\text{ is a } 4\\times 3 \\text{ matrix, while }B=\\begin{bmatrix}1 & 2 \\\\ 3 & 4 \\\\ 5 & 6\\end{bmatrix} \\text{ is a } 3\\times 2 \\text{ matrix.}$$\n",
    "\n",
    "In Python, matrices are create in a similar manner to vectors. We simply give the `array` function a list of lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[2,8,4],[1,0,3],[5,1,6],[8,3,5]])\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = np.array([[1,2],[3,4],[5,6]])\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B[2,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common use of matrices is to digitally encode a picture. Imagine a $100\\times 100$ pixel black and white image. Each pixel encodes the level of brightness at the point, which is just a single number. Writing down the brightness level at each pixel in a $100\\times 100$ grid gives us a $100\\times 100$ matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_matrix = np.load('pixel_matrix.npy')\n",
    "print(pixel_matrix)\n",
    "print(pixel_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try plotting the matrix to see what it represents. We will first need to load in Python's plotting library matplotlib:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(pixel_matrix,cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic operations\n",
    "#### Scalar multiplication\n",
    "As with vectors, if we multiply a matrix by a scalar, we simply multiple every entry in the matrix by that number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A*3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix-vector multiplication\n",
    "\n",
    "We can multiple a $n\\times m$ matrix ($A$) by a $m$-dimensional vector ($x$) and the result will be a $n$-dimensional vector. To perform this multiplication we compute the dot product of each of the rows of $A$ with $x$. The result is a vector with $m$ entries, where the first entry is dot product of the first row of $A$ with $x$, the second entry is dot product of the second row of $A$ with $x$, and so on.\n",
    "\n",
    "<img src=\"https://xaktly.com/Images/Mathematics/MatrixAlgebra/MatrixDefinitions/MatrixDefFigure7.png\" width=600 />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[2],[4],[1]])\n",
    "A.dot(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The order of multiplication matters! We cannot multiple a $m$-dimensional vector by a $n\\times m$ matrix. The number of columns in the first matrix/vector must be the same as the number of rows in the second matrix/vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.dot(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4:** Compute the product $Mu$, where $$M = \\begin{bmatrix}1 & 7 & 3 \\\\ 9 & 6 & 7\\end{bmatrix}\\text{ and }U = \\begin{bmatrix} 2 \\\\ 9 \\\\ 6\\end{bmatrix}$$by hand and then use Python to check your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix multiplication\n",
    "Matrix multiplication is simply an extension of matrix-vector multiplication. When multiplying two matrices we compute the dot product of each row of matrix 1 with each column of matrix 2, and the resulting matrix contains all of these dot products. \n",
    "\n",
    "Lets take our matrices $A$ and $B$ from above and compute their product:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.dot(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiplying a $n\\times m$ matrix by a $m\\times p$ matrix results in a $n\\times p$ matrix ($A$).\n",
    "\n",
    "**Note:** The number of columns in the first matrix must be the same as the number of rows in the second matrix. Hence, the product $BA$ is not defined, as $B$ has 2 columns and $A$ has 4 rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5:** Compute the product $XY$, where $$X = \\begin{bmatrix} 5 & 1 \\\\ 6 & 3\\end{bmatrix}\\text{ and }Y = \\begin{bmatrix} 3 & 7& 2 \\\\ 1 & 4 & 6\\end{bmatrix}$$by hand and then use Python to check your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solving a linear system\n",
    "\n",
    "Suppose we are given the system of equations\n",
    "$$ 2x+y-7z = 1 \\\\ 6x-2y-z = 8 \\\\ 5x-3y+4z = 7 $$\n",
    "and asked to solve them for $x$, $y$ and $z$.\n",
    "\n",
    "This system of equations can be written in matrix form as\n",
    "$$ \\begin{bmatrix} 2 & 1 & 7 \\\\ 6 & -2 & -1 \\\\ 5 & -3 & 4 \\end{bmatrix}\\begin{bmatrix} x \\\\ y \\\\ z \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 8 \\\\ 7 \\end{bmatrix}$$\n",
    "\n",
    "Then we can use the `solve` function from linear algebra module (`linalg`) of NumPy to solve this matrix equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_eq =  np.array([[2, 1, -7], [6, -2, -1], [5, -3, 4]])\n",
    "v_eq = np.array([[1], [8], [7]])\n",
    "solution = np.linalg.solve(M_eq,v_eq)\n",
    "print(solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can manually check the result by plugging in $x=4$, $y=7$ and $z=2$ into the system of equations, or by multiplying the matrix by the solution and verifying that it matches the vector on the right-hand side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_eq.dot(solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving differential equations\n",
    "\n",
    "Now that we know how to store numbers in arrays and perform basic manipulations on these arrays, we can develop tools for studying differential equations.\n",
    "\n",
    "Recall our differential equation for unrestricted population growth $$\\frac{{\\rm d}N}{{\\rm d}t}=a N.$$ This type of equation is called an ordinary differential equation, or ODE for short.\n",
    "\n",
    "This equation can be solved analytically by separating the derivative $\\frac{{\\rm d}N}{{\\rm d}t}$ and integrating both sides:\n",
    "$$\\begin{align}\\int_{N_0}^N\\frac{1}{N^\\prime}{\\rm d^\\prime}N&=\\int_0^t a {\\rm d}t^\\prime\\\\\n",
    "\\log (N) - \\log (N_0)&= at \\\\ N(t) &= N_0*{\\rm e}^{at}\\end{align}$$\n",
    "\n",
    "See [Ordinary differential equation examples](https://mathinsight.org/ordinary_differential_equation_introduction_examples) on Maths Insight for details on how to solce ODEs analytically. Maths is Fun also have a nice tutorial on [First Order Linear Differential Equations](https://www.mathsisfun.com/calculus/differential-equations-first-order-linear.html).\n",
    "\n",
    "Setting our initial population size $N_0$ and growth rate $a$ we can compute the population size at all points in the future:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N0 = 10000\n",
    "a = 0.1\n",
    "t = np.linspace(0,25,101)\n",
    "N = N0*np.exp(a*t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the solution to see how the population size evolves with time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(t,N)\n",
    "plt.xlabel('Time (years)')\n",
    "plt.ylabel('Population size')\n",
    "plt.axis([0,25,10000,120000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, most ODEs do not have nice analytic solutions and we are forced to rely on numerical estimations.\n",
    "See the Wikipedia page [Numerical_methods_for_ordinary_differential_equation](https://en.wikipedia.org/wiki/Numerical_methods_for_ordinary_differential_equations) for an overview of the different numerical methods for solving ODEs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Euler's method\n",
    "Euler's method is the simplest numerical method for estimating the solution of a differential equation. Similar to when we *discretised* the equation for unrestricted population growth and asked what the population increase was each year, Euler's method takes small time steps and computes the increase at each time step. The smaller the time step is, the more accurate the solution.\n",
    "\n",
    "$$x_{n+1} = x_n + h f(x_n,y_n)$$\n",
    "\n",
    "<img src=\"https://teaching.smp.uq.edu.au/scims/Appl_analysis/images/eulerdiagram.png\" width=500 />\n",
    "\n",
    "For the model of unrestricted population growth, the Euler method is defined as \n",
    "$$N_{n+1} = N_n + \\Delta t \\times a N_n,$$\n",
    "where $\\Delta t$ is the length of the time step we take.\n",
    "\n",
    "The first thing we should do is define a function to represent the right-hand side of our equation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dNdt(a,x):\n",
    "    return a*x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we set up the Euler's method and cycle through our time points, estimating the solution at each timepoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = 1\n",
    "t_est = np.arange(0,25+dt,dt)\n",
    "N_est = np.zeros(t_est.size)\n",
    "N_est[0] = N0\n",
    "for j in range(len(t_est)-1):\n",
    "    N_est[j+1] = N_est[j] + dt*dNdt(a,N_est[j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we plot the estimated solution and the exact solution on the same graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(t_est,N_est,label='Estimate')\n",
    "plt.plot(t,N,label='Exact')\n",
    "plt.xlabel('Time (years)')\n",
    "plt.ylabel('Population size')\n",
    "plt.axis([0,25,N0,121000])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining the graph, we see that the estimated solution underestimates the true solution and that it becomes more and more inaccurate as time increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 6:** Change the step size $\\Delta t$ to 0.1 and run the simulation again. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runge-Kutta method\n",
    "\n",
    "We can improve the accuracy of our numerical solver by including higher order term. However, higher\n",
    "order methods require more calculations and function evaluations, and as such, will take longer to run. \n",
    "In practice, a good balance is achieved by the fourth order Runge–Kutta method. Instead of simply using our current population size to estimate, say next years population, we use estimates throughout the time interval (the year) to determine our estimate. The solution at each time point is computed as follows:\n",
    "$$x_{n+1} = x_n + \\frac{1}{6}\\left(k_1+2k_2+2k_3+k_4\\right)$$\n",
    "where\n",
    "\\begin{align}k_1&=\\Delta t f(x_n) \\\\\n",
    "k_2&=\\Delta t f(x_n+\\tfrac{1}{2}k_1) \\\\\n",
    "k_3&=\\Delta t f(x_n+\\tfrac{1}{2}k_2) \\\\\n",
    "k_4&=\\Delta t f(x_n+k_3) \\end{align}\n",
    "\n",
    "See [Harold Serrano's blog post](https://www.haroldserrano.com/blog/visualizing-the-runge-kutta-method) for a nice visualisation of the Runge-Kutta method.\n",
    "\n",
    "As we did for Euler's method, we set up the Runge-Kutta method and iterate over time to compute the estimated solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = 1\n",
    "N_RK = np.zeros(t_est.size)\n",
    "N_RK[0] = N0\n",
    "for n in range(len(t_est)-1):\n",
    "    k1 = dt*dNdt(a,N_RK[n])\n",
    "    k2 = dt*dNdt(a,N_RK[n]+0.5*k1)\n",
    "    k3 = dt*dNdt(a,N_RK[n]+0.5*k2)\n",
    "    k4 = dt*dNdt(a,N_RK[n]+k3)\n",
    "    N_RK[n+1] = N_RK[n] + (k1 + 2*k2 + 2*k3 + k4)/6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plotting the exact solutions and the two estimated solutions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(t,N,label='Exact')\n",
    "plt.plot(t_est,N_est,label='Euler')\n",
    "plt.plot(t_est,N_RK,label='Runge Kutta')\n",
    "\n",
    "plt.xlabel('Time (years)')\n",
    "plt.ylabel('Population size')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the plot, we see that if we use the same time step for both the Euler method and the Runge-Kutta method, the Runge-Kutta method significantly outperforms the Euler method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Built-in ODE solvers\n",
    "\n",
    "In practice, we usually rely on built-in ODE solvers. They are tried and tested functions that will solve a system of ODEs to a high degree of accuracy. These functions will typically be more efficient that one you write yourself.\n",
    "\n",
    "[Scipy's integrate module](https://docs.scipy.org/doc/scipy/reference/integrate.html) contains an array of functions for numerical calculus (numerical differentiation, numerical integration, ODE solvers etc.). You can find a list of all of the ODE solvers included in the integrate module [here](https://docs.scipy.org/doc/scipy/reference/integrate.html#solving-initial-value-problems-for-ode-systems)\n",
    "\n",
    "We will focus on the [solve_ivp function](https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.solve_ivp.html#scipy.integrate.solve_ivp) as it allows us choose from a list of integration methods. The default is the 4th order Runge-Kutta method, with an adaptive step size (updates the step size throughout the simulation, balancing accuracy and efficiency)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.integrate import solve_ivp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use any of the ODE solvers from Scipy's integrate module we must define a function where the first two input arguments are time $t$ and the variable/list of variables $x$, in that order. The system parameter can then be defined as additional input arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dNdt_ivp(t,x,a):\n",
    "    return a*x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The syntax for solve_ivp is `(function name, [t start, t end], initial conditions, args)`. You can include additional arguments to specify the integration method, the tolerance, step size, etc. Read the [function documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.solve_ivp.html) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol = solve_ivp(dNdt_ivp, [0, 25], [N0], args=(a,), dense_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate the solution for an array of time points and plot the solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_ivp = np.linspace(0, 25, 101)\n",
    "N_ivp = sol.sol(t_ivp)\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(t,N,label='Exact')\n",
    "plt.plot(t_est,N_est,label='Euler')\n",
    "plt.plot(t_est,N_RK,label='Runge Kutta')\n",
    "plt.plot(t_ivp,N_ivp[0],label='Built-in')\n",
    "\n",
    "plt.xlabel('Time (years)')\n",
    "plt.ylabel('Population size')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(t,N,label='Exact')\n",
    "plt.plot(t_est,N_RK,label='Runge Kutta')\n",
    "plt.plot(t_ivp,N_ivp[0],label='Built-in')\n",
    "\n",
    "plt.xlabel('Time (years)')\n",
    "plt.axis([24.8,25,119000,122000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** we need to include `dense_output=True` as an additional argument to evaluate the solution on a dense mesh."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 7:** Consider a system described by the following ODE $$\\dot{x}=x(1-x).$$ Define a function `dxdt` for the right-hand side of this expression, using the notation specied above (first two arguments must be $t$ and $x$). Then use the `solve_ivp` function to estimate the solution on the interval $0 \\le t \\le 10$ and initial value $x(0)=0.5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulating the Hodgkin Huxley model\n",
    "\n",
    "Now that we understand what ODEs are and how we can simulate them computationally, we return to the model of Hodgkin and Huxley:\n",
    "\\begin{align}&C_m\\frac{{\\rm d}V}{{\\rm d}t} = I - g_{Na} m^3h (V-V_{Na}) - g_K n^4 (V-V_K) - g_l (V-V_l) \\\\\n",
    "&\\frac{{\\rm d}n}{{\\rm d}t} = \\alpha_n(V)(1-n)-\\beta_n(V)n \\\\\n",
    "&\\frac{{\\rm d}m}{{\\rm d}t} = \\alpha_m(V)(1-m)-\\beta_m(V)m \\\\\n",
    "&\\frac{{\\rm d}h}{{\\rm d}t} = \\alpha_h(V)(1-h)-\\beta_h(V)h\\end{align}\n",
    "\n",
    "First, we set up a function that takes the variables ($V$, $n$, $m$, $h$) as a vector and then returns the right-hand side of each of the equations, again as a vector. We will vary the input current $I$. Hence, why it is included as an input argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HH(t,x,I):\n",
    "    \n",
    "    V = x[0]\n",
    "    n = x[1]\n",
    "    m = x[2]\n",
    "    h = x[3]\n",
    "    \n",
    "    alpha_n = 0.02*(V-25) / (1 - np.exp(-(V-25)/9) )\n",
    "    alpha_m = 0.182*(V+35) / (1 - np.exp(-(V+35)/9))\n",
    "    alpha_h = 0.25*np.exp(-(V+90)/12)\n",
    "    \n",
    "    beta_n = -0.002*(V-25)/(1-np.exp((V-25)/9))\n",
    "    beta_m = -0.124*(V+35)/(1-np.exp((V+35)/9))\n",
    "    beta_h = 0.25*np.exp((V+62)/6) / np.exp((V+90)/12)\n",
    "    \n",
    "    dVdt = 1/C*(I - gNa*m**3*h*(V-VNa) - gK*n**4*(V-VK) - gL*(V-VL))\n",
    "    dndt = alpha_n*(1-n) - beta_n*n\n",
    "    dmdt = alpha_m*(1-m) - beta_m*m\n",
    "    dhdt = alpha_h*(1-h) - beta_h*h\n",
    "    \n",
    "    return [dVdt, dndt, dmdt, dhdt]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the parameters and simulate the model using the `solve_ivp` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "C = 1\n",
    "gNa = 40\n",
    "gK = 35\n",
    "gL = 0.3\n",
    "VNa = 55\n",
    "VK = -77\n",
    "VL = -65\n",
    "\n",
    "# Simulate model\n",
    "HH_sol = solve_ivp(HH, [0,100], [-60,0,0,0.6], dense_output = True, args = (0.2,))\n",
    "t_HH = np.linspace(0, 100, 1000)\n",
    "x_HH = HH_sol.sol(t_HH)\n",
    "plt.plot(t_HH, x_HH[0])\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Voltage')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 8:** Vary the current $I$ (the input argument) and describe how the dynamics of the voltage change as $I$ is increased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kuramoto model\n",
    "\n",
    "Once we begin to look at large populations of neurons, simulating a detailed model, such as the Hodgkin-Huxley model, becomes computationally expensive. Instead, we opt for simple caricatures that capture the important elements of a neuron's dynamics without keeping track of every detail. The Kuramoto model is one such model. It is particularly useful if we wish to understand how neurons *synchronise* their firing times.\n",
    "\n",
    "The Kuramoto model describes the evolution of a neuron's *phase*, how close it is to spiking. The simplest form of the model is given as follows:\n",
    "$$\\frac{{\\rm d}\\theta_i}{{\\rm d}t} = \\omega_i + \\frac{k}{N}\\sum_{j=1}^N \\sin (\\theta_j-\\theta_i),$$\n",
    "where $\\theta_i$ is the phase of the $i$th neuron, $\\omega_i$ is the intrinsic natural frequency of the $i$th neuron, $k$ is the coupling strength and $N$ is the number of neurons in the population. The Wikipedia article on the [Kuramoto model](https://en.wikipedia.org/wiki/Kuramoto_model) provides an nice overview of the model and some of its variations.\n",
    "\n",
    "<img src=\"voltage_phase_conversion.png\" width=600 />\n",
    "\n",
    "As before, we set up a function defining our system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kuramoto(t,x,k):\n",
    "\n",
    "    phase_diff = np.subtract.outer(x, x)\n",
    "\n",
    "    return omega + (k/N) * (np.sin(-phase_diff)).sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need SciPy's statistics module to create our distribution of intrinsic frequencies. We choose the intrinsic frequencies from a Lorentzian with centre $\\omega_0$ and width at half maximum $\\Delta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "k = 1\n",
    "N = 100\n",
    "omega0 = 0.5 \n",
    "Delta = 0.01\n",
    "omega = stats.cauchy.rvs(loc=omega0, scale=Delta, size=N)\n",
    "theta0 = 2*np.pi*np.random.rand(N)\n",
    "\n",
    "# Simulate model\n",
    "kuramoto_sol = solve_ivp(kuramoto, [0,100], theta0, dense_output = True, args = (k,))\n",
    "t_K = np.linspace(0, 100, 1000)\n",
    "x_K = kuramoto_sol.sol(t_K)%(2*np.pi)\n",
    "\n",
    "# Plot\n",
    "plt.figure()\n",
    "plt.plot(t_K, x_K.T)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Phase')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A nice way to visualise the dynamics of Kuramoto oscillators, is to pose them on a circle and watch their phases evolve. To do this, we create an animated plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import module for animations\n",
    "import matplotlib.animation as ani\n",
    "# Configure Jupyter for animated plots\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've copied the code for simulating the Kuramoto model down here so that we can easily re-run it for different coupling strengths and see how this affects the dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate the Kuramoto model\n",
    "k = 0\n",
    "kuramoto_sol = solve_ivp(kuramoto, [0,100], theta0, dense_output = True, args = (k,))\n",
    "x_K = kuramoto_sol.sol(t_K)%(2*np.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up animation\n",
    "theta=np.linspace(0,2*np.pi,1000)\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "\n",
    "ax.set_xlim((-1.1, 1.1))\n",
    "ax.set_ylim((-1.1, 1.1))\n",
    "line, = ax.plot([], [], 'b.', lw=2, markersize=10)\n",
    "\n",
    "def init():\n",
    "    plt.plot(np.cos(theta),np.sin(theta),'k--',linewidth=0.8)\n",
    "    line.set_data([], [])\n",
    "    return (line,)\n",
    "\n",
    "def dots(i):\n",
    "    x = np.cos(x_K[:,i])\n",
    "    y = np.sin(x_K[:,i])\n",
    "    line.set_data(x, y)\n",
    "    return (line,)\n",
    "\n",
    "# Create animation\n",
    "animator = ani.FuncAnimation(fig, dots, init_func=init, interval=100, frames=range(len(t_K)), blit=True, repeat=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For weak coupling, the neurons continue to act independently. As we increase the coupling strength, some of the slower neurons will speed up and some of the faster neurons will slow down. This results in the neurons *synchronising* their activity. For very large values of $k$ all the neurons will synchronise, evolving at the same rate and phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kuramoto order parameter\n",
    "Computing the *order parameter* allows us to quantify how synchronous or asynchronous the population of neurons is.\n",
    "$$ Z = R{\\rm e}^{i\\Psi} = \\frac{1}{N}\\sum_{j=1}^N {\\rm e}^{i\\theta_j}.$$\n",
    "The order parameter is a complex number, whose magnitude $R$ corresponds to the level of synchrony and phase $\\Psi$ is the average phase of the population of neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate the Kuramoto model\n",
    "k = 1\n",
    "kuramoto_sol = solve_ivp(kuramoto, [0,100], theta0, dense_output = True, args = (k,))\n",
    "x_K = kuramoto_sol.sol(t_K)%(2*np.pi)\n",
    "\n",
    "# Compute the Kuramoto order parameter\n",
    "Z = 1/N * np.exp(complex(0, 1) * x_K).sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create an animation that shows the dynamics of the individual neurons as well as the Kuramoto order parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up animation\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "\n",
    "ax.set_xlim((-1.1, 1.1))\n",
    "ax.set_ylim((-1.1, 1.1))\n",
    "\n",
    "line1, = ax.plot([], [], 'b.', lw=2, markersize=10)\n",
    "line2, = ax.plot([], [], color=[0.7,0.7,0.7])\n",
    "dot, = ax.plot([], [], '*', color=[0.7,0.7,0.7], lw=2, markersize=10)\n",
    "\n",
    "def init():\n",
    "    plt.plot(np.cos(theta),np.sin(theta),'k--',linewidth=0.8)\n",
    "    line1.set_data([], [])\n",
    "    line2.set_data([], [])\n",
    "    dot.set_data([], [])\n",
    "    return (line1,line2,dot,)\n",
    "\n",
    "def dots(i):\n",
    "    x =  np.cos(x_K[:,i])\n",
    "    y =  np.sin(x_K[:,i])\n",
    "    line1.set_data(x,y)\n",
    "    line2.set_data(Z[:i].real, Z[:i].imag)\n",
    "    dot.set_data(Z[i].real, Z[i].imag)\n",
    "    return (line1,line2,dot,)\n",
    "    \n",
    "# Create animation\n",
    "animator = ani.FuncAnimation(fig, dots, init_func=init, interval=100, frames=range(len(t_K)), blit=True, repeat=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ott-Antonsen reduction\n",
    "\n",
    "The beauty of the Kuramoto model is that it is amenable to an exact mean field reduction. In the large $N$ limit ($N\\rightarrow \\infty$) the population dynamics can be captured by a single equation that describes the evolution of the order parameter:\n",
    "$$\\frac{{\\rm d}Z}{{\\rm d}t} = (-\\Delta +i\\omega_0)Z + \\frac{1}{2}kZ(1-|Z|^2),$$\n",
    "where the intrinsic frequencies $\\omega_o$ are chosen from a Lorentzian distribution with centre $\\omega_0$ and width at half maximum $\\Delta$.\n",
    "\n",
    "This reduction was derived by [Ott and Antonsen](https://doi.org/10.1063/1.2930766) in 2008, and has since been shown to extend to many other (more complex) phase oscillator models.\n",
    "\n",
    "Remembering that $Z=R{\\rm e}^{i\\Psi}$, we arrive at equations for the synchrony $R$ and average phase $\\Psi$:\n",
    "$$\\begin{align}\\frac{{\\rm d}R}{{\\rm d}t} &= \\left[-\\Delta + \\frac{1}{2}k(1-R^2)\\right]R\\\\\n",
    "\\frac{{\\rm d}\\Psi}{{\\rm d}t}&=\\omega_0\\end{align}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function describing the Kuramoto order parameter in the mean field limit\n",
    "def kuramoto_MF(t,x,k):\n",
    "    \n",
    "    R = x[0]\n",
    "    Psi = x[1]\n",
    "    \n",
    "    dR = (-Delta + 1/2 * k * (1-R**2))*R\n",
    "    dPsi = omega0\n",
    "    \n",
    "    return (dR, dPsi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate the mean-field verion of the Kuramoto model\n",
    "kuramoto_sol = solve_ivp(kuramoto_MF, [0,100], (abs(Z[0]),np.angle(Z[0])), dense_output = True, args = (k,))\n",
    "x_K = kuramoto_sol.sol(t_K)\n",
    "R = x_K[0,:]\n",
    "Psi = x_K[1,:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the mean-field dynamics of the order parameter and the order parameter computed for the simulation of 100 neurons, we see that two are closely matched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(R*np.cos(Psi),R*np.sin(Psi),label='Mean-field')\n",
    "plt.plot(Z.real,Z.imag,label='100 neurons')\n",
    "plt.xlabel('Real(Z)')\n",
    "plt.ylabel('Imag(Z)')\n",
    "plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synaptic plasticity\n",
    "\n",
    "Action potentials are transmitted from one neuron (presynaptic) to another (postsynaptic) at a synapse. When the action potential reaches the synapse the presynaptic neuron releases neurotransmitter, which causes passages to open between the two neurons. These passages are known as ion channels and once open ions can flow in and out of the postsynaptic neuron. Ions have electrical charge, so ions flowing in and out of the neuron is going to lead to a change in voltage. Once the voltage of the postsynaptic neuron increases past a certain value it will release its own action potential and the cycle repeats. \n",
    "\n",
    "Synaptic plasticity refers to the activity-dependent modification of the strength or efficacy of a synapses, i.e. how much of a change in voltage can one action potential create. Such changes in synaptic strength can modify the function of a neural circuit, enabling us to learn and creating memories. See [Citri & Malenka (2007)](https://doi.org/10.1038/sj.npp.1301559) for an overview of synaptic plasticity\n",
    "\n",
    "We will focus on two forms of synaptic plasticity: Hebbian learning and spike-time dependent plasticity (STDP). \n",
    "\n",
    "## Hebbian learning\n",
    "In the 1940s Donald Hebb postulated that *cells that fire together wire together*. The general idea of his theory was that if neuron A and neuron B are regularly activated at the same time, then the synaptic connection between them should gradually become more effective. While if neuron A and neuron B's firing pattern are uncorrelated, the synapse should become less effective. \n",
    "\n",
    "Hebb's theory was later formalised mathematically and there now exist many formulations of Hebbian learning, see  [Gerstner & Kistler (2002)](https://doi.org/10.1007/s00422-002-0353-y) for a nice review. \n",
    "\n",
    "We can create a simple caricature of Hebbian learning, by assigning different coupling strengths $k_{ij}$ for each of the connections and defining a rule that update these coupling strengths based on the phase difference:\n",
    "$$\\begin{align}\\frac{{\\rm d}\\theta_i}{{\\rm d}t} &= \\omega_i + \\frac{1}{N}\\sum_{j=1}^N k_{ij}\\sin (\\theta_j-\\theta_i),\\\\\n",
    "\\frac{{\\rm d}k_{ij}}{{\\rm d}t} &= \\epsilon(\\alpha\\cos(\\theta_j-\\theta_i)-k_{ij}).\\end{align}$$\n",
    "When the phase difference is small, the coupling strength will increase, while if it is large the coupling strength will decrease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kuramoto_plasticity(t,x):\n",
    "    \n",
    "    theta = x[:N]\n",
    "    k = x[N:].reshape(N,N)\n",
    "    \n",
    "    phase_diff = np.subtract.outer(theta, theta)\n",
    "    \n",
    "    dtheta = omega + (1/N) * (k*np.sin(-phase_diff)).sum(axis=1)\n",
    "    dk = epsilon * ( alpha*np.cos(phase_diff) - k )\n",
    "\n",
    "    return np.concatenate((dtheta, dk.reshape(N**2,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "epsilon = 0.5\n",
    "alpha = 1\n",
    "N = 50\n",
    "omega = stats.cauchy.rvs(loc=0.5, scale=0.01, size=N)\n",
    "theta0 = 2*np.pi*np.random.rand(N)\n",
    "k0 = np.random.rand(N,N)\n",
    "x0 = np.concatenate((theta0,k0.reshape(N**2,)))\n",
    "\n",
    "# Simulate\n",
    "kuramoto_sol = solve_ivp(kuramoto_plasticity, [0,100], x0, dense_output = True)\n",
    "t_K = np.linspace(0, 100, 1000)\n",
    "theta_K = kuramoto_sol.sol(t_K)[:N,:]%(2*np.pi)\n",
    "k_ij = kuramoto_sol.sol(t_K)[N:,:]\n",
    "\n",
    "# Compute the Kuramoto order parameter\n",
    "Z = 1/N * np.exp(complex(0, 1) * theta_K).sum(axis=0)\n",
    "\n",
    "# Plot\n",
    "plt.figure()\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(t_K,theta_K.T)\n",
    "plt.ylabel('Phase')\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(t_K,k_ij.T)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Coupling strength')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up animation\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "\n",
    "ax.set_xlim((-1.1, 1.1))\n",
    "ax.set_ylim((-1.1, 1.1))\n",
    "\n",
    "line1, = ax.plot([], [], 'b.', lw=2, markersize=10)\n",
    "line2, = ax.plot([], [], color=[0.7,0.7,0.7])\n",
    "dot, = ax.plot([], [], '*', color=[0.7,0.7,0.7], lw=2, markersize=10)\n",
    "\n",
    "def init():\n",
    "    plt.plot(np.cos(theta),np.sin(theta),'k--',linewidth=0.8)\n",
    "    line1.set_data([], [])\n",
    "    line2.set_data([], [])\n",
    "    dot.set_data([], [])\n",
    "    return (line1,line2,dot,)\n",
    "\n",
    "def dots(i):\n",
    "    x =  np.cos(theta_K[:N,i])\n",
    "    y =  np.sin(theta_K[:N,i])\n",
    "    line1.set_data(x,y)\n",
    "    line2.set_data(Z[:i].real, Z[:i].imag)\n",
    "    dot.set_data(Z[i].real, Z[i].imag)\n",
    "    return (line1,line2,dot,)\n",
    "    \n",
    "# Create animation\n",
    "animator = ani.FuncAnimation(fig, dots, init_func=init, interval=100, frames=range(len(t_K)), blit=True, repeat=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spike-time dependent plasticity (STDP)\n",
    "Spike-time dependent plasticity is an asymmetric version of Hebbian learning. Instead of both synapses ($A\\rightarrow B$ and $B\\rightarrow A$) being strengthened when $A$ and $B$ fire together, one is strengthened and the other is weakened, depending on the order of the spikes. If $A$ spikes and shortly afterward $B$ spikes, the link from $A$ to $B$ is said to be causal and as such the $A\\rightarrow B$ synapse is strengthen while the $B\\rightarrow A$ synapse is weakened as $A$ firing is not causally related to $B$ firing.\n",
    "\n",
    "<img src=\"https://www.researchgate.net/profile/Mojtaba-Madadi-Asl/publication/324705724/figure/fig3/AS:618625325080577@1524503179642/Asymmetric-STDP-learning-window-Spike-timing-window-of-STDP-for-the-induction-of.png\" width=400 />\n",
    "\n",
    "The scholarpedia [Spike-time dependent plasticity](http://www.scholarpedia.org/article/Spike-timing_dependent_plasticity) provides a nice summary of the different mathematical models of STDP.\n",
    "\n",
    "Again, for a simple caricature of STDP we use the same formalism as above, but replace the $\\cos$ in the learning rule with a $\\sin$:\n",
    "$$\\begin{align}\\frac{{\\rm d}\\theta_i}{{\\rm d}t} &= \\omega_i + \\frac{1}{N}\\sum_{j=1}^N k_{ij}\\sin (\\theta_j-\\theta_i),\\\\\n",
    "\\frac{{\\rm d}k_{ij}}{{\\rm d}t} &= \\epsilon(\\alpha\\sin(\\theta_j-\\theta_i)-k_{ij}).\\end{align}$$\n",
    "\n",
    "**Note:** This is a grossly over simplified version of STDP. In particular, with STDP the maximal update of weights should be when the spike-times are very similar. Using a $\\sin$ function means that the maximal update when the phase difference is $\\pm\\pi/2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kuramoto_STDP(t,x):\n",
    "    \n",
    "    theta = x[:N]\n",
    "    k = x[N:].reshape(N,N)\n",
    "    \n",
    "    phase_diff = np.subtract.outer(theta, theta)\n",
    "    \n",
    "    dtheta = omega + (1/N) * (k*np.sin(-phase_diff)).sum(axis=1)\n",
    "    dk = epsilon * ( alpha*np.sin(-phase_diff) - k )\n",
    "\n",
    "    return np.concatenate((dtheta, dk.reshape(N**2,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "epsilon = 0.5\n",
    "alpha = 1\n",
    "N = 50\n",
    "omega = stats.cauchy.rvs(loc=0.5, scale=0.1, size=N)\n",
    "theta0 = 2*np.pi*np.random.rand(N)\n",
    "k0 = np.random.rand(N,N)\n",
    "x0 = np.concatenate((theta0,k0.reshape(N**2,)))\n",
    "\n",
    "# Simulate\n",
    "kuramoto_sol = solve_ivp(kuramoto_STDP, [0,100], x0, dense_output = True)\n",
    "t_K = np.linspace(0, 100, 1000)\n",
    "theta_K = kuramoto_sol.sol(t_K)[:N,:]%(2*np.pi)\n",
    "k_ij = kuramoto_sol.sol(t_K)[N:,:]\n",
    "\n",
    "# Compute the Kuramoto order parameter\n",
    "Z = 1/N * np.exp(complex(0, 1) * theta_K).sum(axis=0)\n",
    "\n",
    "# Plot\n",
    "plt.figure()\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(t_K,theta_K.T)\n",
    "plt.ylabel('Phase')\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(t_K,k_ij.T)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Coupling strength')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up animation\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "\n",
    "ax.set_xlim((-1.1, 1.1))\n",
    "ax.set_ylim((-1.1, 1.1))\n",
    "\n",
    "line1, = ax.plot([], [], 'b.', lw=2, markersize=10)\n",
    "line2, = ax.plot([], [], color=[0.7,0.7,0.7])\n",
    "dot, = ax.plot([], [], '*', color=[0.7,0.7,0.7], lw=2, markersize=10)\n",
    "\n",
    "def init():\n",
    "    plt.plot(np.cos(theta),np.sin(theta),'k--',linewidth=0.8)\n",
    "    line1.set_data([], [])\n",
    "    line2.set_data([], [])\n",
    "    dot.set_data([], [])\n",
    "    return (line1,line2,dot,)\n",
    "\n",
    "def dots(i):\n",
    "    x =  np.cos(theta_K[:N,i])\n",
    "    y =  np.sin(theta_K[:N,i])\n",
    "    line1.set_data(x,y)\n",
    "    line2.set_data(Z[:i].real, Z[:i].imag)\n",
    "    dot.set_data(Z[i].real, Z[i].imag)\n",
    "    return (line1,line2,dot,)\n",
    "    \n",
    "# Create animation\n",
    "animator = ani.FuncAnimation(fig, dots, init_func=init, interval=100, frames=range(len(t_K)), blit=True, repeat=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
